---
layout: post
title:  "Frequently Asked Questions (FAQ) about RL Agent Training."
date:   2021-11-11 12:24:00 +0900
categories: research
tags: reinforcement learning algorithm
author: Phuoc. Pham
comment: true
---

This post contains frequently asked questions you may encounter when working with reinfocement learning. Some of questions are in my research scope and I spent time to figure out and summarize carefully, and some questions aren't as I accidentally saw it somewhere else while browsing for other questions, but I think it will be needed in the future projects. 

The main objective of this post is to serve as a reference for later usage.

**Note:** Most contents of this current post are not written by me. Actually, these are comments from other forums. Therefore, after each quote/opinion, I aslo added links to the original sources for validation.




#### **What is difference between DQN and Policy Gradient methods?**

DQN is a form of Q-learning with *function approximation* (using a neural network), which means it tries to <ins> learn a state-action value function Q</ins> (given by a neural network in DQN) by minimizing TD errors, i.e. trying to make the value  $$Q(s,a)$$  close to  $$r+\gamma max_{a^{'}}Q(s',a)$$  after observing a transition  $$(s,a,r,s')$$ , where the *actions can be chosen arbitrarily* (the algorithm is off-policy), typically using a greedy or epsilon-greedy policy based on the current Q function.

In contrast, policy gradient methods try to <ins>learn a policy function directly</ins> (instead of a Q function), and are typically on-policy, meaning you need to learn from trajectories generated by the current policy. One way to learn an approximation policy is by directly maximizing expected reward using gradient methods, hence "policy gradient" (the gradient of the expected reward w.r.t. the parameters of the policy can be obtained via the policy gradient theorem for stochastic policies, or the deterministic policy gradient theorem[2] for deterministic policies).

[[Source: Quora]](https://www.quora.com/What-is-difference-between-DQN-and-Policy-Gradient-methods)




**Other insights**
- A2C algorithms varies drastically with minor changes in hyperparameters [[ref]](https://cse.buffalo.edu/~avereshc/rl_fall20/Comparison_of_RL_Algorithms_vvelivel_sudhirya.pdf)
- DQN offers better sample efficiency via replay buffer but longer convergence time.[[ref]](https://medium.datadriveninvestor.com/which-reinforcement-learning-rl-algorithm-to-use-where-when-and-in-what-scenario-e3e7617fb0b1)
- DQN alone is unstable and gives poor convergence, hence requires several add-ons.
- DQN is typically more data efficient than ActorCritic. When operating in action spaces that are small, the primitive exploration mechanism of DQN is often good enough to get results. You will expect ActorCritic to take longer to train since it normally requires more data to get good performance due to high variance in the gradient.[[ref]](https://www.reddit.com/r/reinforcementlearning/comments/hmtzqp/actorcritic_vs_dqn/)
- DQN is more sample efficient. If your emulator/problem takes long to compute a transition, it is better to use DQN and friends. If your environment is fast, then actor critic is better with respect to walltime because you take fewer gradient steps. [[ref]](https://www.reddit.com/r/reinforcementlearning/comments/jk83e3/a_significant_difference_in_actorcritic_and_dqn/)
- DQN is the more stable, but slow in term of clock wall time to learn, PPO and A2C are really fast but performance can crash and they are way more sensible to hyper parameters tuning.[[ref]](https://www.reddit.com/r/reinforcementlearning/comments/hxv9we/which_algorithm_to_use/)



#### **What is the differences of #episodes and #steops when training RL agent ?**

They say in paper that they train for 10 million frames which is 10 million timesteps in the environment. 
100 episodes takes roughly 2 hours, and each episode takes maximum 10,000 steps [[ref]](https://ai.stackexchange.com/questions/19911/how-much-time-does-it-take-to-train-dqn-on-atari-environment)
  

(My note): You can consider `#episodes` $$=$$ `#num_of_rollouts`. Some papers refer `#episode` as the number of the model is updated. [[ref]](https://colab.research.google.com/drive/1EmqNyPUPVf8Knvre1SG8dVBtVDqcm-Q4#scrollTo=hnlQrrxELObc)

#### **How many training steps are enough  for Atari Games ?**



- To give context, the original paper : Playing with Atari Games, trains each model for 200M environment steps [[ref]](https://towardsdatascience.com/learnings-from-reproducing-dqn-for-atari-games-1630d35f01a9)
- More than 1M steps: [[ref]](https://towardsdatascience.com/learnings-from-reproducing-dqn-for-atari-games-1630d35f01a9)
- However, some of the best model-free reinforcement learning algorithms require tens or hundreds of millions of time steps â€“ the equivalent of several weeks of training in real time. , we explore training with a budget restricted to 100K time steps [[ref]](https://openreview.net/pdf?id=S1xCPJHtDB)
- Atari 2600: average total reward after training for 50 M time steps. [[ref]](https://www.researchgate.net/figure/Atari-2600-average-total-reward-after-training-for-50-M-time-steps-Boldface-numbers_tbl1_310329157)
- Training our algorithm for 5,000,000 learning batches (100 training epochs) [[ref]]https://www.diva-portal.org/smash/get/diva2:1341574/FULLTEXT01.pdf
- We train our models for the RAM state space for 1M steps and our models for the image frame state space for 2M steps.[[ref]](https://nihit.github.io/resources/spaceinvaders.pdf)
- `train_steps = 50M, buffer_size=1M` [[ref]](https://github.com/msinto93/DQN_Atari/blob/master/train.py)
- Trained for 10 million examples/frames per game. [[ref]](https://github.com/aleju/papers/blob/master/neural-nets/Playing_Atari_with_Deep_Reinforcement_Learning.md)
- `n_timesteps: !!float 1e7` [[ref]](https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/hyperparams/dqn.yml)
- For [A2C](https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/hyperparams/a2c.yml), it needs around \~1M training steps.



#### **How much memory required for training DQN/Rainbow on Atari Games?**

**Q**: DQN and Rainbow for Atari, the experience replay size is 1 million, how much RAM are these algorithms need? 

**A**: If you store the frames in uint8, the whole replay buffer takes about 6-8GB of memory.


#### **How large should the `replay_buffer` be? ?**

The larger the experience replay, the less likely you will sample correlated elements, hence the more stable the training of the NN will be. However, a large experience replay also requires a lot of memory and it might slow training. So, there is a trade-off between training stability (of the NN) and memory requirements. [[ref]](https://ai.stackexchange.com/questions/11640/how-large-should-the-replay-buffer-be)




#### **Why are correlated samples bad in DQN ?**


The reason you randomly sample from the replay buffer is to reduce the sequential aspect of each $$(s,a,r,s')$$ pair. Imagine an agent learning to drive a car; If you train a network based on 100 samples of only uphill driving, the gradient of the training step over compensates for pushing on the accelerator (a given action).
That action becomes overly reinforced such that higher Q values are likely to prevail regardless of other states.
If we break up those sequential patterns, the gradient is likely to train the network across all actions.

Well say A2C and A3C handle this by running many copies of the env in parallel. The variety of experience from the multiple envs addresses the perfectly correlated samples issue.
ACER takes that further by including experience replay along with AC. [[ref]](https://www.reddit.com/r/reinforcementlearning/comments/alua6f/why_are_correlated_samples_bad_in_dqn/)

Experience replay is a key technique behind many recent advances in deep reinforcement learning. Allowing the agent to learn from earlier memories can speed up learning and break undesirable temporal correlation. [[ref]](https://proceedings.allerton.csl.illinois.edu/2018/media/files/0091.pdf)


#### **What is the `explained_variance` in the `A2C` training log ?**

The explained variance score explains the *dispersion of errors of a given dataset*, and the formula is written as follows:

$$explained\_variance(y, \hat{y}) == 1 - \frac{Var(y-\hat{y})}{Var(y)}$$

Here, $$Var(y-\hat{y})$$ and $$Var(y)$$ is the variance of prediction errors and actual values respectively. Scores close to 1.0 are highly desired, indicating better squares of standard deviations of errors. And vice versa, lower values are worse.

Referces:
1. [Explained variance score as a risk metric](https://www.oreilly.com/library/view/mastering-python-for/9781789346466/d1ac368a-6890-45eb-b39c-2fa97d23d640.xhtml)
2. [Explained variance score](https://scikit-learn.org/stable/modules/model_evaluation.html#explained-variance-score)
