---
layout: post
title:  "Life 3.0: Being Human in the Age of Artificial Intelligence"
date:   2021-11-21 21:10:04 +0900
categories: book_summary 
tags: life AGI intelligence consciousness
author: Phuoc. Pham
comment: true
---

This article is a brief summarization of the book "*Life 3.0: Being Human in the Age of Artificial Intelligence*"  (Max Tegmark ). It took me about  2 weeks to read this wonderful book. This post is serve as a reference for future usage.


#### **Chapter 1: The Tale of Prometheus Team**: 
(*it is a beautiful story, you should read the full version of it*)


#### **Chapter 2 : Introduction**

**The three stages of Life**
What is a definition of life: a process that can retain its complexity and replicate. In other words, it's a self-replicating information-processing system whose information (software) determines both its behavior and the blueprints for its hardware.


- **Life 1.0**: (biological stage) Life where both the hardware and software are evolved rather designed. E.g: Bacteria aren't great learners. They can not update it's software to move toward sugar.
- **Life 2.0**: (cultural stage) Life whose hardware is evolved, but whose software is largely designed 
- **Life 3.0**: (technological stage); designs its hardware and software.

**Controversies about Life 3.0**

- **Digital Utopianism**: Digital life is the natural and desirable next step in the cosmic evolution and that if we let digital minds be free rather than try to sop or enslave them, the outcome is almost certain to be good.
- **Techno-skeptics**: They think that building super-human AGI is so hard that it won't happen for hundreds of years and therefore view it as silly to worry about it now. + worrying about AI risk was a potentially harmful distraction that could slow the progress of AI.
- **The Beneficial-AI Movement**: Human-level AGI in this century is a real possibility and a good outcome isn't guaranteed + Try to make safety-AI research / beneficial-AI movement go to the mainstream, not AI-risk.

**Misconceptions**
The goal of this book is to help you join the AI conversation about  the future needs to continue.
Timeline myths, controversy myths (they don't fully understand what other people mentioning), myths about what the risks are.

#### **Chapter 3: Matter Turns Intelligent ?**
Intelligence = ability to accomplish complex goals.
Today, humans win hands-down on breadth, while machines outperform us in a small but growing number of narrow domains.
The landscape of human competence (the ability to do something successfully or efficiently.)

**What Is Memory ?**
The fundamental physical property of memory: can be in many different long-lived states
Information can take on a life of its own, independent of its physical substrate !
Memory of computers need "addresses" to be access, human memory works as a search engine (auto-associative)

**What Is Computation ?**
The idea of substrate independence.
Hardware is the matter and the software is the pattern.
Moore Law still be true in this century. (replacing with the new technology)


- Intelligence, defined as ability to accomplish complex goals, can't be measured by a single IQ number, only by an ability spectrum across all goals.
- Today's artificial intelligence tends to be narrow, with each system able to accomplish only very specific goals, while human intelligence is remarkably board.
- Memory, computation, learning and intelligence have an abstract, intangible and ethereal feel to them because they're substrate-independent: able to take on life of their own that doesn't depend on or reflect the details of their underlying material substrates.
- Any chunk of matter can be the substrate for memory as long as it has many different stable states.
- Any matter can be computronium, the substrate for computation, as long as it contains certain universal building blocks that can be combined to implement any function. NAND gates and neurons are two important examples of such universal "computational atoms."
- A neural network is a powerful substrate for learning, because simply by obeying the laws of physics, it can rearrange itself to get better and better at implementing desired functions.
- Because of the striking simplicity of the laws of physics, we human only care about a tiny fraction of all imaginable computational problems, and neural networks tend to be remarkably good at solving precisely this tiny fraction.
- Once technology gets twice as powerful, it can often be used  to design and build technology that's twice as powerful in turn, triggering repeated capability doubling in the spirit of Moore's Law. The cost of information technology has now halved roughly every two years for about a century, enabling the information age.
- If AI progress continues, then long before AI reaches human level for all skills, it will give us fascinating opportunities and challenges involving issues such  as bugs, laws, weapons and jobs - which we'll explore in the next chapter.



#### **Chapter 3: The Near Future: Breakthroughs, Bugs, Laws, Weapons and Jobs**

- Near-team AI progress has the potential to greatly improve our lives in myriad ways, from making our personal lives, power girds, and financial grids and financial markets more efficient to saving lives with self-driving cars, surgical bots and AI diagnosis systems.
- We we allow real-world systems to be controlled by AI, it's crucial that we learn to make AI more robust, doing what we want it to do. This boils down to solving tough technical problems related to verification, validation, security and control.
- This need for improved robustness is particularly pressing for AI-controlled weapons, where the stakes can be huge.
- Many leading AI researchers and roboticists have called for an international treaty banning certain kinds of autonomous weapons, to avoid an out-of-control arms race that could end up marking convenient assassination machines available to everybody with a full wallet and an axe to grind.
- AI can mek our legal systems more fair and efficient if we can figure out how to make robojudges transparent and unbiased.
- Our laws need rapid updating to keep up with AI, which poses tough legal questions involving privacy, liability and regulation.
Long before we need to worry about intelligent machines replacing us altogether, they may increasingly replace us on the job market.
- This need not be a bad thing, as long as society redistributes a fraction of the AI-created wealth to make everyone better off.
- Otherwise, amny economists argue, inequality will greatly increase.
With advance planning, a low-employment society should be able to flourish not only financially, with people getting their sense of purpose from activities other than jobs
- Career advice for today's kids: Go into professions that machines are bad at - those involving people, unpredictability and creativity.
- There's a non-negligible possibility that AGI progress will proceed to human levels and beyond-we'll explore that in the next chapter !

- Bugs and Robust AI: Human relies on the same tried-and-true approach to keeping our technology beneficial. But we'll inevitably reach a point where even a single accident could be devastating enough to outweigh all benefits.
- AI for Space Exploration
- AI for Finance: We need to do validation,  as verification asks "Did I build the system right?", validation asks "Did I build the right system?". For example, does the system rely on assumptions that might not always valid ? If so, how it can be improved to better handle uncertainty?
- Weapons: Human-in-the-loop is more better approach., but is is not alway the ideal approach.


#### **Chapter 4: Intelligence Explosion ?**

- If we one day succeed in building human-level AGI, this may trigger an intelligence explosion, leaving us far behind.
- If a group of humans manage to control an intelligence explosion, they may be able to take over the world in a matter of years.
- If humans fail to control an intelligence explosion, the AI itself may take over the world even faster.
Whereas a rapid intelligence explosion is likely to lead to a single world power, a slow one dragging on for years or decades may be more likely to lead to a multipolar scenario with a balance of power between a large number of rather independent entities.
- The history of life shows it self-organizing into an ever more complex hierarchy shaped by collaboration, competition and control. Superintelligence is likely to enable coordination on ever-larger cosmic scales, but it's unclear whether it will ultimately lead to more totalitarian top-down control or more individual empowerment.
- Cyborgs and uploads are plausible, but arguably not the fastest route to advanced machine intelligence.
- The climax of our current race toward AI may be either the best or the worst thing ever to happen to humanity, with a fascinating spectrum of possible outcomes that we'll explore in the next chapter.
- We need to start thinking hard about which outcome we prefer and how to steer in that direction, because if we don't know what we want, we're unlikely to get it.
- The story of elephants and humans (We don't threat humans, so why would they kill us ?)
- Bigger = slower (wat a lay with your hand?)
- The better the communication and transportation technology gets , the larger hierarchies can grow.

#### **Chapter 5: Aftermath: The Next 10000 Years**
- The current race toward AGI can end in a fascinatingly broad range of aftermatch scenarios for upcoming millennia.
- Superintelligence can peacefully coexist with humans either because it's forced to (enslaved-god scenario) or because it's "friendly AI" that wants to (libertarian-utopia, protector-god, benevolent-dictator and zookeeper scenarios).
- Superintelligence can be prevented by an AI (gatekeeper scenario) ỏ by humans (1984 scenario), by deliberately forgetting the technology (reversion scenario) or by lack of  incentives to build it (egalitarian-utopia scenario)
- Humanity can go extinct and get replaced by ÁI (conqueror and descendant scenarios) ỏ by nothing (self-destruction scenario).
- There's absolutely no consensus on which, if any, of these scenarios are desirable, and all involve objectionable elements. This makes it all the more important to continue and deepen the conversation around the future goals, so that we don't inadvertently drift or steer in an unfortunate direction. 


#### **Chapter 6: Our Cosmic Endowment: The Next Billion Years and Beyond**
- Compared to cosmic timescales of billions of years, an intelligence explosion is a sudden event where technology rapidly plateaus at level limited only by the laws of physics.
- This technological plateau is vastly higher than today's technology, allowing a given amount of matter to generate about 10 billion times more energy (using sphalerons or black holes), store 12-18 orders of magnitude more information or compute 31-41 orders of magnitude faster - or to be converted to any other desired form of matter.
- Superintelligent life would not only make such dramatically more efficient use of its existing resources, but would also be able to grow today;'s biosphere by about 32 orders of magnitude by acquiring more resources through cosmic settlement at near light speed.
- Dark energy limits the cosmic expansion of superintelligent life also protects it from distant expanding death bubbles or hostile civilizations. The threat of dark energy tearing cosmic civilizations apart motivates massive cosmic engineering projects, including wormhole construction if this turns out to be feasible.
- The main commodity shared or traded across cosmic distances is likely to be information.
- Barring wormholes, the light-speed limit on communication poses severe challenges for coordination and control across a cosmic civilization. A distant central hub may incentivize its superintelligent "nodes" to cooperate either through rewards or through threats, say by deploying a local guard AI programmed to destroy the node by setting off a supernova or quasar unless the rules are obeyed. 
- The collision of two expanding civilizations may result in assimilation, cooperation or war, where the latter is arguably less likely than it is between today's civilizations.
- Despite popular belief to the contrary, it is quite plausible that w're the only life form capable of making our observable Universe come alive in the future.
- If we don't improve our technology, the question isn't whether humanity will go extinct, but merely how: will an asteroid, a supervolcano, the burning heat of the aging Sun or some other calamity get us first ?
- If we do keep improving our technology with enough care, foresight and planing to avoid pitfalls, life has the potential to flourish on Earth and far beyond for many billions of years, beyond the wildest dreams of our ancestors.


#### **Chapter 7: Goal**
- The ultimate origin of goal-oriented behavior lies in the laws of physics, which involve optimization.
Thermodynamics has the built-in goal of dissipation: to increase a measure of messiness that's called entropy.
- Life is phenomenon that can help dissipate (increase overall messiness) even faster by retaining or growing its complexity and replicating while increasing the nessiness of its environment.
- Darwinian evolution shifts the goal-oriented behavior from dissipation to replication.
- Intelligence is the ability to accomplish complex goals.
- Since we humans don't always have the resources to figure out the truly optimal replication strategy, we've evolved useful of thumb that guide our decisions: feelings such as hunger, thirst, pain, lust, and compassion.
- We therefore no longer have a simple goal such as replication; when our feelings conflict with the goal of our genes, we obey our feelings as by using birth control.
- We're building increasingly intelligent machines to help us accomplish our goals. In so far as we build such machines to exhibit goal-oriented behavior, we strike to align the machine goals with ours.
- Aligning machine goals with our own involves three unsolved problems: making machines learn them, adopt them and retain them.
- AI can be created to have virtually any goal, but almost any sufficiently ambitious goal can lead to subgoals of self-preservation, resource acquisition and curiosity to understand the world better - the former two may potentially lead a superintelligent AI to cause problems for humans, and the latter may prevent it from retaining the goals we give it.
- Although many broad ethical principles are agreed upon by most humans, it's unclear how to apply them to other entities, such as non-human animals and future AIs.
- It's unclear how to imbue a superintelligent AI with an ultimate goal that neither is undefined nor leads to the elimination of humanity, making it timely to rekindle research on some of the thorniest issues in philosophy!


#### **Chapter 8: Consciousness**

- There's no undisputed definition of "consciousness." I used the board and non-anthropocentric definition consciousness = subjective experience.
- Whether AIs are conscious in that sense is what matters of the thorniest ethical and philosophical problems posed by the rise of AI:  Can AI suffer? Should they have rights ? Is uploading a subjective suicide ? Could a future cosmos teeming with AIs be the ultimate zombie apocalypse?
- The problem of understanding intelligence shouldn't be conflated with three seperate problems of consciousness: the "pretty hard problem" of predicting which physical systems are conscious, the "even harder problem" of predicting qualia, and the "really hard problem" of why anything at all is conscious.
- The "pretty hard problem" of consciousness is scientific, since a theory that predicts which of your brain processes are conscious is experimentally testable and falsifiable, while it's currently unclear how science could fully resolve the two harder problems.
- Neuroscience experiments suggest that many behaviors and brain regions are unconscious, which much of our conscious experience representing an after-the-fact summary of vastly larger amounts of unconscious information.
- Generalizing consciousness predictions from brains to machines requires a theory. Consciousness appears to require not a particular kind of particle or field, but a particular kind of information processing that's fairly autonomous and integrated, so that the whole system is rather autonomous but its parts aren't.
- Consciousness might feel so non-physical because it's doubly substrate-independent: if consciousness is the way information feels when being processed in certain complex ways, then it's merely the structure of the information processing that matters, not the structure of the matter doing the information processing.
- If artificial consciousness is possible, then the space of possible AI experiences is likely to be huge compared to what we humans can experience, spanning a vast spectrum of qualia and timescales-all sharing a feeling of having free will.
- Since there can be no meaning without consciousness, it's not our Universe giving meaning to conscious beings, but conscious beings giving meaning to our Universe.
- This suggests that as we humans prepare to be humbled by ever smarter machines, we take comfort mainly in being Homo sentiens, not Homo sapiens.

**Other notes:**
- You can convert many routines from conscious to unconscious through extensive practice.
- If two individual consciousnesses suddenly disappear and get replaced by a single unified one or would the transition be gradual so that the individual consciousnesses coexisted in some form even á a joint experience began to emerge ?


#### **References**
1. [Amazon Book, Hardcover – Deckle Edge, August 29, 2017](https://www.amazon.com/Life-3-0-Being-Artificial-Intelligence/dp/1101946598)
2. [Max Tegmark lecture on Life 3.0 – Being Human in the age of Artificial Intelligence](https://www.youtube.com/watch?v=1MqukDzhlqA)