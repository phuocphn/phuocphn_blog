---
layout: post
title:  "Towards Scale-Invariant Graph-related Problem Solving by Iterative Homogeneous Graph Neural Networks"
date:   2022-02-19 15:00:00 +0900
categories: paper_summaries
tags: graph neural network
author: Phuoc. Pham
comments: true
---

### **Introduction**

Current GNNs lack generalizability with respect to scales (graph sizes, graph diameters, edge weights, etc..) when solving many graph analysis problems.  Specifically, we are interested in GNNs that can learn from small graphs and perform well on new graphs of arbitrary scales. Existing GNNs are either ineffective or inefficient under this setting. In fact, even ignoring the optimization process of network training, the representation power of existing GNNs is yet too limited to achieve graph scale generalizability. 

There are at least two issues: 
1. <ins>By using a pre-defined layer number, these GNNs ARE NOT ABLE to approximate graph algorithms whose complexity depends on graph size </ins> (most graph algorithms in textbooks are of this kind). The reason is easy to see: For most GNNs, each node only uses information of the 1-hop neighborhoods to update features by message passing, and it is impossible for k-layer GNNs to send messages between nodes whose distance is larger than k.  Loukas proves that *GNNs, which fall within the **message passing framework**, lose a significant portion of their power for solving many graph problems when their width and depth are restricted;*

2. <ins>A not-so-obvious observation is that, the range of numbers to be encoded by the internal representation may deviate greatly for graphs of different scales</ins>. For example, if we train a GNN to solve the shortest path problem on small graphs of diameter $$k$$ with weight in the range of $$[0, 1],$$ the internal representation could only need to build the encoding for the path length within $$[0, k]$$, but if
we test this GNN on a large graph of diameter $$K >> k$$ with the same weight range, then it has to use and transform the encoding for $$[0, K]$$. The performance of classical neural network modules (e.g. the multilayer perceptron in GNNs) are usually highly degraded on those out-of-range inputs.



To address the Problem $$\#1$$, we take a program synthesis perspective, to design GNNs that have stronger representation power by mimicking the control flow of classical graph algorithms. 

Typical graph algorithm, such as Dijkstra’s algorithm for shortest path computation, are ***iterative***. They often consist of two sub-modules: 
- **an iteration body** to solve the sub-problem (e.g., update the distance for the neighborhood of a node as in Dijkstra), 
- **a termination condition** to control the loop out of the iteration body. 

By adjusting the iteration numbers, an iterative algorithm can handle arbitrary large-scale problems. We, therefore, introduce our novel **Iterative GNN** (IterGNN) that equips ordinary GNN with an **adaptive and differentiable stopping criterion** to let GNN iterate by itself, as shown in Figure 1. Our stopping condition is adaptive to the inputs, supports arbitrarily large iteration numbers, and, interestingly, is able to be trained in an end-to-end fashion without anydirect supervision.

<!-- A partial solution to address the issue of out-of-range number encoding is provided, if the underlying graph algorithm is in a specific hypothesis class. More concretely, the solutions to many graph problems, such as the shortest path problem and TSP problem, are homogeneous w.r.t the input graph weights, i.e., the solution scales linearly with the magnitudes of the input weights. -->

> Homogeneous is used to describe a group or thing which has members or parts that are all the same.


To build GNNs with representation power to approximate the solution to such graph problems, we further introduce **the homogeneous inductive-bias**. By *assuming the message processing functions are homogeneous*, the knowledge that neural networks learn at one scale can be generalized to different
scales. We build **HomoMLP and HomoGNN** as powerful approximates of homogeneous functions over vectors and graphs, respectively.


### **Method**
This section is constructed as follows:

- *(First section)* We propose **Iterative GNN (IterGNN)** to enable adaptive and unbounded iterations of GNN layers so that the model can generalize to graphs of arbitrary scale. 
- *(Second section)* We further introduce **HomoGNN** to partially solve the problem of out-of-range number encoding for graph-related problems. 
- *(Third section)* We finally describe **PathGNN** that improves the generalizability of GNNs for distance-related problems by improving the algorithm alignments to the Bellman-Ford algorithm.


#### **Iterative GNN (IterGNN)**

The core of IterGNN is a differentiable iterative module. It *executes the same GNN layer repeatedly* until a learned stopping criterion is met. We present the pseudo-codes in the Algorithm 1.

<p align="center" >
  <img src="https://i.imgur.com/6NnBNoU.png" width="500">
</p>


At time step $$k$$, the iteration body $$f$$ updates the hidden states as $$h^k = f(h^{k−1}))$$; the stopping criterion function $$g$$ then calculates a confidence score $$c^k = g(h^k) \in [0, 1]$$ to describe <ins>the probability of the iteration to terminate at this step</ins>. The module determines the number of iterations using a random process based on the confidence scores $$c^k$$


At each time step k, the random process has a probability of $$c^k$$ to terminate the iteration and to return the current hidden states $$h^k$$ as the output. The probability for the whole process to return $$h^k$$ is then $$p^k = (\prod_{i=1}^{k-1}(1 − c^i)) c^k$$, which is the product of the probabilities of continuing the iteration at steps from $$1$$ to $$k − 1$$ and stopping at step $$k$$. 

However, the sampling procedure is not differentiable. Instead, we execute the iterative module until the “continue” probability $$\prod_{i=1}^{k=1}(1-c^i)$$  is smaller than a threshold $$\mathcal{E}$$ and return an expectation $$h =\sum_{j=1}^{k}p^jh^j$$ at the end. 


The gradient to the output $$h$$ thus can optimize the hidden states $$h^k$$ and the confidence scores $$c^k$$ jointly.


Our iterative module can resemble the control flow of many classical graph algorithms since the iteration of most graph algorithms depends on the size of the graph. For example, Dijkstra’s algorithm has a loop to greedily propagate the shortest path from the source node. The number of iterations to run the loop depends linearly on the graph size. Ideally, we hope that our $$f$$ can learn the loop body and $$g$$ can stop the loop when all the nodes have been reached from the source. Interestingly, the experiment result shows such kind behavior. 


#### **HomoGNN**

The homogeneous prior is introduced to improve the generalizability of GNNs for out-of-range features/attributes. 

We first define the positive homogeneous property of a function:

***Definition 1***. 
- A function $$f$$ over vectors is positive homogeneous iff $$f(λ\overrightarrow{x}) = λf(\overrightarrow{x})$$ for all $$λ > 0$$.
- A function $$f$$ over graphs is positive homogeneous iff for any graph $$G = (V, E)$$ with node attributes $$\overrightarrow{x}_v$$ and edge attributes $$\overrightarrow{x}_e$$

$$f(G, {λ\overrightarrow{x}_v : v \in V }, {λ\overrightarrow{x}_e : e \in E}) = λf(G, {\overrightarrow{x}_v : v ∈ V }, {\overrightarrow{x}_e : e \in E})$$

The solutions to most graph-related problems are positive homogeneous, such as the length of the
shortest path, the maximum flow, graph radius, and the optimal distance in the traveling salesman
problem.

**The homogeneous prior tackles the problem of different magnitudes of features for generalization.**
As illustrated in Figure 2, by assuming functions as positive homogeneous, models can generalize knowledge to the scaled features/attributes of different magnitudes. For example, let us assume two datasets $$D$$ and $$D\lambda$$ that are only different on magnitudes, which means $$D\lambda := {\lambda x : x \in D}$$ and $$λ > 0$$. If the target function $$f$$ and the function $$F_A$$ represented by neural networks $$A$$ are both homogeneous, the prediction error on dataset $$D\lambda$$ then scales linearly w.r.t. the scaling factor $$λ.$$

**We design the family of GNNs that are homogeneous, named HomoGNN**, as follows: simply remove all the bias terms in the multi-layer perceptron (MLP) used by ordinary GNNs, so that <ins>all affine transformations</ins> degenerate to <ins>linear transformations</ins>. Additionally, only homogeneous activation functions are allowed to be used. Note that ReLU is a homogeneous activation function. *The original <ins>MLP</ins> used in ordinary <ins>GNNs</ins> become <ins>HomoMLP</ins> in <ins>HomoGNNs</ins> afterward.*

#### **PathGNN**

We design PathGNN to imitate one iteration of the classical *Bellman-Ford algorithm*. It inherits the generalizability of the Bellman-Ford algorithm and the flexibility of the neural networks. Specifically, the Bellman-Ford algorithm performs the operation:

$$dist_i = min(dist_i , min_{j\in N(i)}(dist_j + w_{ji}))$$

iteratively to solve the shortest path problem, where $$dist_i$$ is the current estimated distance from the source node to the node $$i$$, and $$w_{ji}$$  denotes the weight of the edge from node $$j$$ to node $$i$$. 


<p align="center" >
  <img src="https://i.imgur.com/tTjiIle.png" width="500">
</p>


If we consider $$dist_i$$ as node features and $$w_{ij}$$ as edge features, one iteration of the Bellman-Ford algorithm can be exactly reproduced by GNN layers as described in Eq. 1 of the manuscript.

To achieve more flexibilities for solving problems other than the shortest path problem, we integrate neural network modules, such as MLPs to update features or the classical attentional-pooling to aggregate features, while building the PathGNN layers. A typical variant of PathGNN is as follows:

<p align="center" >
  <img src="https://i.imgur.com/pUf6xr3.png" width="500">
</p>


### **Experiments**

Our experimental evaluation aims to study the following empirical questions: 

1. Will our proposals, (*the PathGNN layer + the homogeneous prior + and the iterative module*) improve the generalizability of GNNs with respect to graph scales that are the number of nodes, the diameter of graphs, and the
magnitude of attributes? 
2. Will our **iterative module** adaptively change the iteration numbers and consequently learn an interpretable stopping criterion in practice? 
3. Can our proposals improve the performance of general graph-based reasoning tasks such as those in physical simulation, image-based
navigation, and reinforcement learning?


#### **Graph theory problems and tasks.**

We consider three graph theory problems, i.e., shortest path, component counting, and Traveling Salesman Problem (TSP), to evaluate models’ generalizability w.r.t. graph scales. We build a benchmark by combining multiple graph generators, so that the generated graphs can have more diverse properties. 

We further apply our proposals to three graph-related reasoning tasks, i.e., physical simulation, symbolic Pacman, and image-based navigation.

#### **Models and baselines.**

Previous problems and tasks can be formulated as *graph regression/classification problems*. We thus construct models and baselines following the common practice. We stack 30 GCN/GAT layers to build the baseline models. GIN is not enlisted since 30-layer GINs do not converge in most of our preliminary experiments. 

- “Path” model stacks 30 PathGNN layers. 
- “Homo-Path” model replaces GNNs and MLPs in the “Path” model with HomoGNNs and HomoMLPs. 
- “Iter-Path” model adopts the iterative module to control the iteration number of the GNN layer in the “Path” model. 
- The final “Iter-Homo-Path” integrates all proposals together.

#### **Main Result: Solving graph theory problems**

**Generalize w.r.t. graph sizes and graph diameters**. We present the generalization performance for all three graph theory problems in Table 1. 
<p align="center" >
  <img src="https://i.imgur.com/EvRToLJ.png" width="900">
</p>



Models are trained on graphs of sizes within [4, 34) and are evaluated on graphs of larger sizes such as 100 (for shortest path and TSP) and 500 (for
component counting so that the diameters of components are large enough). 

The relative loss metric is defined as $$| y − \hat{y} |/|y|$$, given a label $$y$$ and a prediction $$\hat{y}$$. **The results demonstrate that each of
our proposals improves the generalizability on almost all problems**. 

Exceptions happen on graphs generated by Erdos-Renyi (ER). It is because the diameters of those graphs are 2 with high probability even though the graph sizes are large. 

Our final model, **Iter-Homo-Path, which integrates all proposals, performs much better than the baselines such as GCN and GAT**. The performance on graphs generated by KNN and PL further supports the analysis. 

~

We then explore models’ generalizability on much larger graphs on the shortest path problem using Lob to generate graphs with larger diameters. As shown in Table 2, our model achieves a 100% success rate of identifying the shortest paths on graphs with as large as 5000 nodes even though
it is trained on graphs of sizes within [4, 34). **As claimed, the iterative module is necessary for generalizing to graphs of much larger sizes and diameters due to the message passing nature of GNNs**. The iterative module successfully improves the performance from ∼ 60% to 100% on graphs of sizes $$\geq$$ 500.

<p align="center" >
  <img src="https://i.imgur.com/Fv47rTC.png" width="900">
</p>

## **Implementation Understanding**

### class **_GNNLayer**
![https://i.imgur.com/9Yomh4x.png](https://i.imgur.com/9Yomh4x.png)

- param: `input_feat_flag` in `__init__` function: (datatype: boolean), meaning ?
- function `call_node_feat`, with 1 param: `data`, if `input_feat_flag` is `True` then doing concatination between `data.input_x` and `data.x` in the last axis.

**Q**: What is `data.input_x` and `data.x` ?


There are three variants of PathGNN, i.e. MPNN-Max, PathGNN, and PathGNN-sim, each of which corresponds to different degrees of flexibilities. In our experiments, they perform much better than GCN and GAT for all path-related tasks regarding the generalizability.

### class **MPNNConv** 

![https://i.imgur.com/0vQeN7m.png](https://i.imgur.com/0vQeN7m.png)

inherited from `torch_geometric.MessagePassing` 

- param: `in_channel`: similar to other layer.
- param: `out_channel`: similar to other layer.
- param: `edge_channel`: DON'T KNOW ?
- param: `homogenous_flag`: (datatype: boolean, meaning: DON'T KNOW ?)
- param: `edge_embedding_layer_num=2`: DON'T KNOW ?
  - it is used for build `self.edge_embedding_module` and `mid_channel`. `self.edge_embedding_module` is used in `message` function.
- param: `update_layer_num=0`: DON'T KNOW ?
  - it is used for build `self.update_module`. `self.update_module` is used in `update` function.


### class **MPNNMaxConv**
It just a partial class for `MPNNConv` with the fixed value `aggr=max`.

### class **PathConv** + **PathSimConv**
inherited from `nn.Module`


Another variant of PathGNN is also designed by exploiting a less significant inductive bias of the Bellman-Fold algorithm. Specifically, we observe that **only the sender node’s attributes** and **the edge attributes** are useful in the message module while approximating the Bellman-Fold algorithm. Therefore, *we only feed those attributes into the message module of our new PathGNN variant*,

**PathGNN-sim**.
![https://i.imgur.com/9f6RWoC.png](https://i.imgur.com/9f6RWoC.png)


- param: `in_channel`
- param: `out_channel`
- param: `edge_channel`
- param: `homogeneous_flag`
- param: `edge_embedding_layer_num`

Diferences:
- Assertion:
  - **PathConv**: `assert(edge_embedding_layer_num > 0 or out_channel==in_channel*2+edge_channel)`
  - **PathSimConv**: `assert(edge_embedding_layer_num > 0 or out_channel==in_channel+edge_channel)`

**Open Questions**: 
1. Why we have the number of `2` in **PathConv** ?
2. What is the meaning of using `edge_attr_values` and `edge_attr_keys` ?


### class **PathGNNLayers**

inherited from `_GNNLayer`

- param: `layer_name`: str , default value='MPNNMaxConv'
- param: `x_dim`: None
- param: `input_x_dim`: None
- param: `output_x_dim`: None
- param: `edge_attr_dim`: None,
- param: `input_feat_flag`: default = False
- param: `homogeneous_flag`: defaul = False


In `_one_test_case(layer_generator)` function, there is a line to make a "fake" data for these parameters:

```
x_dim, input_x_dim, edge_attr_dim, output_x_dim = np.random.randint(1, 100, size=4).tolist()
output_x_dim = x_dim
```


```python
self.gnn_module = globals()[layer_name](
  x_dim + (input_x_dim if input_feat_flag else 0),
  output_x_dim, edge_attr_dim,
  homogeneous_flag=homogeneous_flag)
```

in `forward` function

`x = self.gnn_module(x, edge_index, edge_attr)`


![https://i.imgur.com/Vf9HSEt.png](https://i.imgur.com/Vf9HSEt.png)

<!-- ![https://i.imgur.com/x38yLrE.png](https://i.imgur.com/x38yLrE.png) -->


### class **\_generate_data**

![https://i.imgur.com/lNNz5rq.png](https://i.imgur.com/lNNz5rq.png)
**Q**: What is the meaning of `input_x` and what is the difference between `input_x` and `x` ?

**A**: `x_dim` will be added with `input_x_dim` if `input_feat_flag` is enabled.

Wrong assertition

(input_x_dim is None and input_feat_flag)



*(this post is still updating ...)*
